{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac699c3",
   "metadata": {},
   "source": [
    "# Evaluate Overlapped Speech Detection (OSD) and Voice Activity Detection (VAD)\n",
    "\n",
    "evaluate OSD/VAD predictions obtained via wav2vec2_audioFrameClassification.py or wav2vec2_audioFrameClassification_multitask.py\n",
    "\n",
    "evaluation is done via pyannote.metrics (https://pyannote.github.io/)\n",
    "\n",
    "evaluation on AMI corpus supports pyannote.db.odessa.ami (https://github.com/pyannote/pyannote-db-odessa-ami) - \n",
    "use task=\"AMI_OSD_dev\" / \"AMI_VAD_dev\" (ODESSA/AMI development set) or task=\"AMI_OSD_test\" / \"AMI_VAD_test\" (ODESSA/AMI test set)\n",
    "\n",
    "other datasets require reference annotations in text format (one txt file per audio):\n",
    "\n",
    "- the format we use is a sequence of space-delimited integers, 100 values per second (one for every 10ms of audio); first value is the total number of labeled audio frames, then 1 = overlap/speech; 0 = non-overlap/non-speech (e.g. \"61500 0 0 0 0 1 1 1 1 0 0 0 \\[...\\]\")\n",
    "\n",
    "(Note: many of the function names and variables refer to \"overlaps\", but the same code also works for VAD (or any other similar detection task), as long as the references are in the same format)\n",
    "\n",
    "### How to use this notebook:\n",
    "- run all cells until you reach \"Evaluation starts here\"\n",
    "- pick one of the options for eval (custom references or pyannote.db)\n",
    "- change the paths and settings there and run the cells\n",
    "\n",
    "### Changelog:\n",
    "2023-02-17\n",
    "- fixed miss/FA values when evaluating using pyannote.db.odessa.ami - previously, the miss/FA values were accidentally swapped (watch out for this when loading previous saved results!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5129b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Annotation, Timeline, Segment\n",
    "from pyannote.metrics.detection import (\n",
    "    DetectionCostFunction,\n",
    "    DetectionErrorRate,\n",
    "    DetectionPrecision,\n",
    "    DetectionRecall,\n",
    "    DetectionAccuracy,\n",
    "    DetectionPrecisionRecallFMeasure\n",
    ")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090062c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global settings\n",
    "\n",
    "MIN_OL_LEN = 0 # min. duration of detected overlaps/speech intervals - anything shorter is ignored\n",
    "MIN_NON_OL_LEN = 0 # min. duration of gaps between two overlaps/speech intervals - if shorter, they are merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads OSD or VAD references from a text file\n",
    "#  (the format we use is a sequence of space-delimited integers, 100 values per second (one for every 10ms of audio);\n",
    "#  first value is the total number of labeled audio frames, then 1 = overlap/speech; 0 = non-overlap/non-speech\n",
    "#   (e.g. \"61500 0 0 0 0 1 1 1 1 0 0 0 [...]\")\n",
    "\n",
    "def get_overlap_reference(ref_file,frameRate=100,offset=0):\n",
    "\n",
    "    with open(ref_file,'r') as file:\n",
    "        ref = file.readline().strip().split(\" \")\n",
    "    \n",
    "    if len(ref) == 0:\n",
    "        print(\"Failed to read anything from file %s\"%(ref_file))\n",
    "    \n",
    "    ref = list(map(float, ref))\n",
    "\n",
    "\n",
    "    num_values = int(ref[0])\n",
    "\n",
    "    uem = Segment(0,num_values / frameRate + offset)\n",
    "    \n",
    "    reference = get_annotation_from_labels(ref[1:],frameRate,offset,apply_min_lens=False)\n",
    " \n",
    "    return reference,uem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e527d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts filenames of audio segments into unique identifiers of the original audio files\n",
    "\n",
    "def get_audio_id(filename):\n",
    "    # obtains the ID of an audio file by repeatedly removing specific substrings from the end\n",
    "    #  (and only the end, to avoid false matches)\n",
    "    #  (e.g. \"IS1009c.Mix-Headset_t700-710.wav\" -> \"IS1009c\")\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    suffixesToRemove = ['.wav','.mp4','.dereverb','.denoise','.OMEETING',\n",
    "                        '.OHALLWAY','.OOFFICE','.meeting','.booth','.office',\n",
    "                        '_OMEETING','_OHALLWAY','_OOFFICE','_meeting','_booth','_office',\n",
    "                        '.Mix-Headset'] \n",
    "    \n",
    "    regexToRemove = ['(\\.|_)\\d+kHz',\n",
    "                    '_t[0-9]+(\\.[0-9]+)?\\-[0-9]+(\\.[0-9]+)?']\n",
    "    \n",
    "    found = True\n",
    "    while found:\n",
    "        found = False\n",
    "        for suffix in suffixesToRemove:\n",
    "            if basename.endswith(suffix):\n",
    "                basename = basename[:-(len(suffix))]\n",
    "                found = True\n",
    "        for rgx in regexToRemove:\n",
    "            match = re.search(rgx+\"$\",basename) # +\"$\" ensures we only find a match at the *end* \n",
    "            if match is not None:\n",
    "                basename = basename[:match.start()]\n",
    "                found = True\n",
    "    \n",
    "    return basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88906317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get OSD or VAD hypotheses for each threshold, in pyannote's preferred format\n",
    "\n",
    "def get_annotation_from_labels(labels,frameRate,offset=0,threshold=0.5,annotation=None,apply_min_lens=False):\n",
    "    num_values = len(labels)\n",
    "    \n",
    "    if annotation is None:\n",
    "        annotation = Annotation()\n",
    "        \n",
    "    overlaps_list = []\n",
    "    \n",
    "    nOverlaps = 0\n",
    "    \n",
    "    isOverlap = False\n",
    "    for i in range(num_values):\n",
    "        if not isOverlap and labels[i] >= threshold:\n",
    "            startTime = i / frameRate + offset\n",
    "            isOverlap = True\n",
    "        if isOverlap and labels[i] < threshold:\n",
    "            endTime = i / frameRate + offset\n",
    "            isOverlap = False\n",
    "            \n",
    "            if apply_min_lens and nOverlaps > 0 and startTime - overlaps_list[-1][1] < MIN_NON_OL_LEN: # if the interval between overlaps is too short\n",
    "                overlaps_list[-1][1] = endTime # merge the overlaps\n",
    "            else:\n",
    "                overlaps_list.append([startTime,endTime]) # else add a new overlap\n",
    "            \n",
    "    if isOverlap: # if the file ends with an overlap, add it too\n",
    "        endTime = num_values / frameRate + offset\n",
    "        if apply_min_lens and nOverlaps > 0 and startTime - overlaps_list[-1][1] < MIN_NON_OL_LEN: # if the interval between overlaps is too short\n",
    "            overlaps_list[-1][1] = endTime # merge the overlaps\n",
    "        else:\n",
    "            overlaps_list.append([startTime,endTime]) # else add a new overlap\n",
    "        \n",
    "    for startTime,endTime in overlaps_list:\n",
    "        if (not apply_min_lens) or endTime - startTime >= MIN_OL_LEN:\n",
    "            annotation[Segment(startTime, endTime)] = 'overlap'\n",
    "        \n",
    "    return annotation\n",
    "\n",
    "def get_overlap_hypotheses_w2v2(test_file,frameRate=50,audio_id_list=None,offset=0.02,\n",
    "                                threshold=0.5,duration=None,shift=None):\n",
    "    # reads predicted labels from w2v2 output, stitches them back together \n",
    "    # and turns them into hypotheses in the format required by pyannote\n",
    "    \n",
    "    predictions_all = []\n",
    "    \n",
    "    with open(test_file,'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            path,values_str = line.split(\",\")\n",
    "            audio_filename = os.path.basename(path)\n",
    "            \n",
    "            audio_id = get_audio_id(audio_filename)\n",
    "            if (audio_id_list is not None) and not (audio_id in audio_id_list):\n",
    "                continue # skip files that are not on the list\n",
    "            \n",
    "            match = re.search('_t[0-9]+(\\.[0-9]+)?\\-[0-9]+(\\.[0-9]+)?',audio_filename)\n",
    "            if match is not None:\n",
    "                str = match.group()\n",
    "                startTime,endTime = str[2:].split('-')\n",
    "                startTime = float(startTime)\n",
    "                endTime = float(endTime)\n",
    "            else:\n",
    "                startTime = None\n",
    "                endTime = None\n",
    "\n",
    "            idx1 = values_str.find('[')\n",
    "            idx2 = values_str.find(']')\n",
    "            \n",
    "            values = values_str[idx1+1:idx2].split(\" \")\n",
    "            values = list(map(float, values))\n",
    "            \n",
    "            predictions_all.append({\n",
    "                \"path\":path,\n",
    "                \"values\": values,\n",
    "                \"id\": audio_id,\n",
    "                \"startTime\": startTime,\n",
    "                \"endTime\": endTime\n",
    "            })\n",
    "            \n",
    "    labels_all = {}\n",
    "    for prediction in predictions_all:\n",
    "        audio_id = prediction[\"id\"]\n",
    "        if audio_id in labels_all:\n",
    "            startTime = prediction[\"startTime\"]\n",
    "            endTime = prediction[\"endTime\"]\n",
    "            if startTime is not None and endTime is not None:\n",
    "                \n",
    "                startIdx = round(startTime * frameRate)\n",
    "                endIdx = round(endTime * frameRate)\n",
    "          \n",
    "                if duration is not None and shift is not None:\n",
    "                    overlap = duration - shift # this \"overlap\" refers to overlap between audio segments\n",
    "                    halfoverlap_frames = round(shift * frameRate / 2)\n",
    "                    startIdx = startIdx + halfoverlap_frames\n",
    "                    \n",
    "                    if startIdx >= endIdx: # this can happen in the very last segment\n",
    "                        #print(\"start time %f, end time %f\"%(startTime,endTime))\n",
    "                        continue # just skip this file completely\n",
    "                    \n",
    "                    newLabels = prediction[\"values\"][halfoverlap_frames:]\n",
    "                    \n",
    "                    if endIdx - startIdx != len(newLabels):\n",
    "                        # w2v labels are one value shorter than time*frameRate => off by one will probably happen every time...\n",
    "                        #  ... and the last segment of each conversation will obviously have a much bigger difference\n",
    "                        endIdx = startIdx + len(newLabels)\n",
    "                    labels_all[audio_id][startIdx:endIdx] = newLabels\n",
    "                else:\n",
    "                    if len(prediction[\"values\"]) != endIdx - startIdx:\n",
    "                        endIdx = startIdx + len(prediction[\"values\"])\n",
    "                    labels_all[audio_id][startIdx:endIdx] = prediction[\"values\"]\n",
    "            else:\n",
    "                # Note: \"+ [0]\" adds the one missing label (w2v outputs only 999 labels for 20s audio)\n",
    "                labels_all[audio_id] = labels_all[audio_id] + prediction[\"values\"] + [0]\n",
    "        else:\n",
    "            labels_all[audio_id] = prediction[\"values\"]\n",
    "            \n",
    "            \n",
    "    hypotheses = {}\n",
    "    for audio_id in labels_all:\n",
    "        hypotheses[audio_id] = get_annotation_from_labels(labels_all[audio_id],\n",
    "                frameRate=frameRate,offset=offset,threshold=threshold,apply_min_lens=True)\n",
    "    \n",
    "    return hypotheses\n",
    "            \n",
    "    \n",
    "def get_overlap_hypotheses_CNN(test_dir,frameRate,offset,threshold=0.5,audio_id_list=None):\n",
    "    # reads predicted labels from our old CNN-based overlap detection (Kunesova et al., 2019)\n",
    "    # and turns them into hypotheses in the format required by pyannote\n",
    "    \n",
    "    overlaps = []\n",
    "    \n",
    "    for filename in os.listdir(test_dir):\n",
    "        if filename.endswith('.bin'):\n",
    "            path = os.path.join(test_dir,filename)\n",
    "            with open(path,'rb') as file:\n",
    "                values = np.fromfile(file, dtype=np.float32)\n",
    "                \n",
    "                match = re.search('_t[0-9]+(\\.[0-9]+)?\\-[0-9]+(\\.[0-9]+)?',filename)\n",
    "                if match is not None:\n",
    "                    str = match.group()\n",
    "                    startTime,endTime = str[2:].split('-')\n",
    "                    startTime = float(startTime)\n",
    "                    endTime = float(endTime)\n",
    "                else:\n",
    "                    startTime = 0\n",
    "                    endTime = None\n",
    "                \n",
    "                audio_id = os.path.basename(filename)\n",
    "                audio_id = get_audio_id(audio_id[0:-4])\n",
    "                \n",
    "                if (audio_id_list is not None) and not (audio_id in audio_id_list):\n",
    "                    continue\n",
    "                    \n",
    "                #print(audio_id)\n",
    "\n",
    "                overlaps.append({\n",
    "                    \"path\":path,\n",
    "                    \"values\": values,\n",
    "                    \"id\": audio_id,\n",
    "                    \"startTime\": startTime\n",
    "                })\n",
    "\n",
    "    hypotheses = {}   \n",
    "      \n",
    "    for overlap in overlaps:\n",
    "        audio_id = overlap[\"id\"]\n",
    "        if audio_id in hypotheses:\n",
    "            hypotheses[audio_id] = get_annotation_from_labels(overlap[\"values\"],\n",
    "                frameRate,offset + overlap[\"startTime\"],threshold,\n",
    "                annotation=hypotheses[audio_id],apply_min_lens=True)\n",
    "        else:\n",
    "            hypotheses[audio_id] = get_annotation_from_labels(overlap[\"values\"],\n",
    "                frameRate,offset + overlap[\"startTime\"],threshold,apply_min_lens=True)\n",
    "    return hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision, recall, etc for all files and thresholds from a specific set of hypotheses\n",
    "#  (version with custom references in text format)\n",
    "def evaluate_overlaps(hypotheses,thresholds,main_thresh,ref_suffix=\"_overlaps.txt\"):\n",
    "    \n",
    "    nThresh = len(thresholds)\n",
    "    \n",
    "    detectionErrorRate = DetectionErrorRate()\n",
    "    detectionAccuracy = DetectionAccuracy()\n",
    "    detectionPrecision = DetectionPrecision()\n",
    "    detectionRecall = DetectionRecall()\n",
    "    detectionFMeasure = DetectionPrecisionRecallFMeasure()\n",
    "\n",
    "    results = [[\"threshold\",\"error_rate\",\"accuracy\",\"precision\",\"recall\",\"fscore\",\"TP-FP delta\"]]\n",
    "    results_pf = [[\"audio_id\",\"error_rate\",\"accuracy\",\"precision\",\"recall\",\"fscore\",\"TP-FP delta\"]]\n",
    "    reference_all = None\n",
    "\n",
    "    miss_total = [0] * nThresh\n",
    "    FA_total = [0] * nThresh\n",
    "    miss_pctg_total = [0] * nThresh\n",
    "    FA_pctg_total = [0] * nThresh\n",
    "    overlap_total = [0] * nThresh\n",
    "    DetErr_avg = [0] * nThresh\n",
    "\n",
    "    TN_total = [0] * nThresh\n",
    "    TP_total = [0] * nThresh\n",
    "    FN_total = [0] * nThresh\n",
    "    FP_total = [0] * nThresh\n",
    "    accuracy_avg = [0] * nThresh\n",
    "    TP_FP_delta = [0] * nThresh\n",
    "    TP_FP_delta_total = [0] * nThresh\n",
    "    TP_FP_delta_avg = [0] * nThresh\n",
    "\n",
    "    retrieved_total = [0] * nThresh\n",
    "    relevant_total = [0] * nThresh\n",
    "    RR_total = [0] * nThresh\n",
    "    precision_avg = [0] * nThresh\n",
    "    recall_avg = [0] * nThresh\n",
    "\n",
    "    fscore_avg = [0] * nThresh\n",
    "    \n",
    "    DetErr_total = [0] * nThresh\n",
    "    accuracy_total = [0] * nThresh\n",
    "    precision_total = [0] * nThresh\n",
    "    recall_total = [0] * nThresh\n",
    "    fscore_total = [0] * nThresh\n",
    "    \n",
    "    frames_total = [0] * nThresh\n",
    "    \n",
    "    nfiles = 0\n",
    "\n",
    "    for audio_id in hypotheses[0].keys():\n",
    "        print(audio_id)\n",
    "        \n",
    "        ref_file = os.path.join(REF_DIR, audio_id + ref_suffix)\n",
    "        if not os.path.exists(ref_file):\n",
    "            print('File ''%s'' not found. Skipping.'%ref_file)\n",
    "            continue\n",
    "        \n",
    "        nfiles = nfiles + 1\n",
    "        \n",
    "        reference,uem = get_overlap_reference(ref_file,frameRate=100,offset=0)\n",
    "        \n",
    "        for iThresh in range(nThresh): \n",
    "\n",
    "            # Detection Error Rate\n",
    "\n",
    "            hypothesis = hypotheses[iThresh][audio_id]\n",
    "\n",
    "            error_rate_details = detectionErrorRate(reference, hypothesis, detailed=True, uem=uem)\n",
    "            error_rate = error_rate_details[\"detection error rate\"]\n",
    "\n",
    "            miss_total[iThresh] = miss_total[iThresh] + error_rate_details[\"miss\"]\n",
    "            FA_total[iThresh] = FA_total[iThresh] + error_rate_details[\"false alarm\"]\n",
    "            overlap_total[iThresh] = overlap_total[iThresh] + error_rate_details[\"total\"]\n",
    "            DetErr_avg[iThresh] = DetErr_avg[iThresh] + error_rate\n",
    "\n",
    "            # Accuracy\n",
    "\n",
    "            accuracy_details = detectionAccuracy(reference, hypothesis, detailed=True, uem=uem)\n",
    "            accuracy = accuracy_details[\"detection accuracy\"]\n",
    "\n",
    "            TP = accuracy_details[\"true positive\"]\n",
    "            FP = accuracy_details[\"false positive\"]\n",
    "            TN = accuracy_details[\"true negative\"]\n",
    "            FN = accuracy_details[\"false negative\"]\n",
    "            TP_FP_delta = TP - FP\n",
    "            \n",
    "            TN_total[iThresh] = TN_total[iThresh] + TN\n",
    "            TP_total[iThresh] = TP_total[iThresh] + TP\n",
    "            FN_total[iThresh] = FN_total[iThresh] + FN\n",
    "            FP_total[iThresh] = FP_total[iThresh] + FP\n",
    "            accuracy_avg[iThresh] = accuracy_avg[iThresh] + accuracy\n",
    "            TP_FP_delta_avg[iThresh] = (TP - FP) / (TP + FP + TN + FN)\n",
    "            \n",
    "            TP_FP_delta_total[iThresh] += TP_FP_delta\n",
    "            \n",
    "\n",
    "            # Precision\n",
    "\n",
    "            precision_details = detectionPrecision(reference, hypothesis, detailed=True, uem=uem)\n",
    "            precision = precision_details[\"detection precision\"]\n",
    "\n",
    "            retrieved_total[iThresh] = retrieved_total[iThresh] + precision_details[\"retrieved\"]\n",
    "            RR_total[iThresh] = RR_total[iThresh] + precision_details[\"relevant retrieved\"]\n",
    "            precision_avg[iThresh] = precision_avg[iThresh] + precision\n",
    "\n",
    "            # Recall\n",
    "\n",
    "            recall_details = detectionRecall(reference, hypothesis, detailed=True, uem=uem)\n",
    "            recall = recall_details[\"detection recall\"]\n",
    "\n",
    "            relevant_total[iThresh] = relevant_total[iThresh] + recall_details[\"relevant\"]\n",
    "            recall_avg[iThresh] = recall_avg[iThresh] + recall\n",
    "\n",
    "            # F-score\n",
    "\n",
    "            fscore_details = detectionFMeasure(reference, hypothesis, detailed=True, uem=uem)\n",
    "            fscore = fscore_details[\"F[precision|recall]\"]\n",
    "\n",
    "            fscore_avg[iThresh] = fscore_avg[iThresh] + fscore\n",
    "\n",
    "            #print(error_rate_details)\n",
    "            #print(accuracy_details)\n",
    "            #print(precision_details)\n",
    "            #print(recall_details)\n",
    "            #print(fscore_details)\n",
    "            #print(costFcn_details)\n",
    "            \n",
    "            if thresholds[iThresh] == main_thresh:\n",
    "                results_pf.append([audio_id,error_rate,accuracy,precision,recall,fscore,TP_FP_delta])\n",
    "\n",
    "\n",
    "\n",
    "    #nfiles = len(hypotheses[0])\n",
    "    \n",
    "    for iThresh in range(nThresh): \n",
    "\n",
    "        DetErr_avg[iThresh] /= nfiles\n",
    "        accuracy_avg[iThresh] /= nfiles\n",
    "        precision_avg[iThresh] /= nfiles\n",
    "        recall_avg[iThresh] /= nfiles\n",
    "        fscore_avg[iThresh] /= nfiles\n",
    "        TP_FP_delta_avg[iThresh] /= nfiles\n",
    "        \n",
    "        frames_total[iThresh] = TP_total[iThresh] + TN_total[iThresh] + FP_total[iThresh] + FN_total[iThresh]\n",
    "        \n",
    "        \n",
    "        TP_FP_delta_total[iThresh] /= frames_total[iThresh]\n",
    "\n",
    "        if overlap_total[iThresh] == 0:\n",
    "            if (miss_total[iThresh] + FA_total[iThresh]) == 0:\n",
    "                DetErr_total[iThresh] = 0\n",
    "            else:\n",
    "                DetErr_total[iThresh] = 9999999\n",
    "        else:\n",
    "            DetErr_total[iThresh] = (miss_total[iThresh] + FA_total[iThresh]) / overlap_total[iThresh]\n",
    " \n",
    "        accuracy_total[iThresh] = (TP_total[iThresh] + TN_total[iThresh]) / (TP_total[iThresh] + TN_total[iThresh] + FP_total[iThresh] + FN_total[iThresh])\n",
    "    \n",
    "        miss_pctg_total[iThresh] = FN_total[iThresh] / overlap_total[iThresh]\n",
    "        FA_pctg_total[iThresh] = FN_total[iThresh] / overlap_total[iThresh]\n",
    "    \n",
    "    \n",
    "        if retrieved_total[iThresh] == 0:\n",
    "            precision_total[iThresh] = 1\n",
    "        else:\n",
    "            precision_total[iThresh] = RR_total[iThresh] / retrieved_total[iThresh]\n",
    "            \n",
    "        if relevant_total[iThresh] == 0:\n",
    "            recall_total[iThresh] = 1\n",
    "        else:\n",
    "            recall_total[iThresh] = RR_total[iThresh] / relevant_total[iThresh]\n",
    "            \n",
    "        if (precision_total[iThresh] + recall_total[iThresh]) == 0:\n",
    "            fscore_total[iThresh] = 0\n",
    "        else:\n",
    "            fscore_total[iThresh] = 2 * (precision_total[iThresh] * recall_total[iThresh]) / (precision_total[iThresh] + recall_total[iThresh])\n",
    "\n",
    "        if thresholds[iThresh] == main_thresh:\n",
    "            results_pf.append([\"AVG\",DetErr_avg[iThresh],accuracy_avg[iThresh],precision_avg[iThresh],recall_avg[iThresh],fscore_avg[iThresh],TP_FP_delta_avg[iThresh]])\n",
    "            results_pf.append([\"TOTAL\",DetErr_total[iThresh],accuracy_total[iThresh],precision_total[iThresh],recall_total[iThresh],fscore_total[iThresh],TP_FP_delta_total[iThresh]])\n",
    "\n",
    "        results.append([thresholds[iThresh],DetErr_total[iThresh],accuracy_total[iThresh],precision_total[iThresh],recall_total[iThresh],fscore_total[iThresh],TP_FP_delta_total[iThresh]])\n",
    "\n",
    "    \n",
    "    stats = {}\n",
    "    stats[\"thresholds\"] = thresholds\n",
    "    #stats[\"error_rate\"] = DetErr_avg\n",
    "    #stats[\"accuracy\"] = accuracy_avg\n",
    "    #stats[\"precision\"] = precision_avg\n",
    "    #stats[\"recall\"] = recall_avg\n",
    "    #stats[\"fscore\"] = fscore_avg\n",
    "    \n",
    "    stats[\"error_rate\"] = DetErr_total\n",
    "    stats[\"accuracy\"] = accuracy_total\n",
    "    stats[\"precision\"] = precision_total\n",
    "    stats[\"recall\"] = recall_total\n",
    "    stats[\"fscore\"] = fscore_total\n",
    "    stats[\"TP-FP delta\"] = TP_FP_delta_total\n",
    "    stats[\"miss\"] = miss_pctg_total\n",
    "    stats[\"FA\"] = FA_pctg_total\n",
    "    \n",
    "    return results, stats, results_pf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate VAD or OSD precision, recall, etc for all files and thresholds from a specific set of hypotheses\n",
    "#  (version using pyannote.db.odessa.ami, AMI corpus only)\n",
    "# FIXED 2023-02-17: previously, the miss/FA values were accidentally swapped\n",
    "from pyannote.database import get_protocol\n",
    "\n",
    "def evaluate_AMI(hypotheses,thresholds,main_thresh,task):\n",
    "    \n",
    "    print('Evaluation with pyannote refs')\n",
    "    \n",
    "    protocol = get_protocol('AMI.SpeakerDiarization.MixHeadset')\n",
    "    \n",
    "    detectionErrorRate = DetectionErrorRate()\n",
    "    detectionAccuracy = DetectionAccuracy()\n",
    "    detectionPrecision = DetectionPrecision()\n",
    "    detectionRecall = DetectionRecall()\n",
    "    detectionFMeasure = DetectionPrecisionRecallFMeasure()\n",
    "    \n",
    "    # references for OSD have to be converted, otherwise the function evaluates VAD\n",
    "    task_is_overlap = (task in [\"AMI_OSD_dev\",\"AMI_OSD_test\"])\n",
    "    \n",
    "    results = [[\"threshold\",\"error_rate\",\"accuracy\",\"precision\",\"recall\",\"fscore\",\"miss\", \"FA\"]]\n",
    "    \n",
    "    DetErr_total = []\n",
    "    accuracy_total = []\n",
    "    precision_total = []\n",
    "    recall_total = []\n",
    "    fscore_total = []\n",
    "    miss_total = []\n",
    "    FA_total = []\n",
    "    \n",
    "    nThresh = len(thresholds)\n",
    "    for iThresh in range(nThresh): \n",
    "        if thresholds[iThresh] == main_thresh: \n",
    "            display = True\n",
    "            print(\"Results for threshold %g:\"%main_thresh)\n",
    "        else:\n",
    "            display = False\n",
    "\n",
    "        if task[-4:] == \"_dev\":\n",
    "            AMI_set = protocol.development()\n",
    "        else:\n",
    "            AMI_set = protocol.test()\n",
    "            \n",
    "        # iterate over each file of the test/dev set\n",
    "        for test_file in AMI_set:\n",
    "            \n",
    "            audio_id = get_audio_id(test_file[\"uri\"])\n",
    "            #print(audio_id)\n",
    "            \n",
    "            hypothesis = hypotheses[iThresh][audio_id]\n",
    "            \n",
    "            # evaluate hypothesis\n",
    "            if task_is_overlap:\n",
    "                # references for overlap\n",
    "                reference = to_overlap(test_file)\n",
    "            else:\n",
    "                # references for VAD\n",
    "                reference = test_file['annotation']\n",
    "            \n",
    "            uem = test_file['annotated']\n",
    "            detectionErrorRate(reference, hypothesis, uem=uem)\n",
    "            detectionAccuracy(reference, hypothesis, uem=uem)\n",
    "            detectionPrecision(reference, hypothesis, uem=uem)\n",
    "            detectionRecall(reference, hypothesis, uem=uem)\n",
    "            detectionFMeasure(reference, hypothesis, uem=uem)\n",
    "            \n",
    "        err = detectionErrorRate.report(display=display)\n",
    "        acc = detectionAccuracy.report(display=display)\n",
    "        prec = detectionPrecision.report(display=display)\n",
    "        rec = detectionRecall.report(display=display)\n",
    "        F = detectionFMeasure.report(display=display)\n",
    "        \n",
    "        err = err.values.tolist()\n",
    "        DetErr_total.append(err[-1][0])  \n",
    "        FA_total.append(err[-1][3]) # FIXED 2023-02-17: previously, the miss/FA values were accidentally swapped\n",
    "        miss_total.append(err[-1][5]) \n",
    "        \n",
    "        acc = acc.values.tolist()\n",
    "        accuracy_total.append(acc[-1][0])\n",
    "        \n",
    "        prec = prec.values.tolist()\n",
    "        precision_total.append(prec[-1][0])\n",
    "        \n",
    "        rec = rec.values.tolist()\n",
    "        recall_total.append(rec[-1][0])\n",
    "        \n",
    "        F = F.values.tolist()\n",
    "        fscore_total.append(F[-1][0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        results.append(\n",
    "            [thresholds[iThresh],DetErr_total[iThresh],accuracy_total[iThresh],\n",
    "             precision_total[iThresh],recall_total[iThresh],fscore_total[iThresh],\n",
    "             miss_total[iThresh],FA_total[iThresh]]\n",
    "        )\n",
    "        \n",
    "        detectionErrorRate.reset()\n",
    "        detectionAccuracy.reset()\n",
    "        detectionPrecision.reset()\n",
    "        detectionRecall.reset()\n",
    "        detectionFMeasure.reset()\n",
    "        \n",
    "        \n",
    "    stats = {}\n",
    "    stats[\"thresholds\"] = thresholds\n",
    "    \n",
    "    stats[\"error_rate\"] = DetErr_total\n",
    "    stats[\"accuracy\"] = accuracy_total\n",
    "    stats[\"precision\"] = precision_total\n",
    "    stats[\"recall\"] = recall_total\n",
    "    stats[\"fscore\"] = fscore_total\n",
    "        \n",
    "    return results,stats,None\n",
    "\n",
    "\n",
    "# Note: to_overlap() is taken from pyannote/metrics/cli.py by H. Bredin\n",
    "#   (https://github.com/pyannote/pyannote-metrics/blob/develop/pyannote/metrics/cli.py)\n",
    "def to_overlap(current_file: dict) -> Annotation: \n",
    "    \"\"\"Get overlapped speech reference annotation \n",
    "  \n",
    "    Parameters \n",
    "    ---------- \n",
    "    current_file : `dict` \n",
    "        File yielded by pyannote.database protocols. \n",
    "  \n",
    "    Returns \n",
    "    ------- \n",
    "    overlap : `pyannote.core.Annotation` \n",
    "        Overlapped speech reference. \n",
    "        \n",
    "    \"\"\" \n",
    "  \n",
    "    reference = current_file[\"annotation\"] \n",
    "    overlap = Timeline(uri=reference.uri) \n",
    "    for (s1, t1), (s2, t2) in reference.co_iter(reference): \n",
    "        l1 = reference[s1, t1] \n",
    "        l2 = reference[s2, t2] \n",
    "        if l1 == l2: \n",
    "            continue \n",
    "        overlap.add(s1 & s2) \n",
    "    return overlap.support().to_annotation() \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e04c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate multiple sets of predictions\n",
    "#   (stats are saved so that they don't need to be recalculated every time)\n",
    "# TODO: check if thresholds match the saved ones, recalculate (only) the missing stats if not\n",
    "\n",
    "import pickle\n",
    "\n",
    "def evaluate_all(prediction_files, thresholds=None, ref_suffix=\"_overlaps.txt\",\n",
    "                main_thresh=0.5,audio_id_list=None,duration=20,shift=10,\n",
    "                stats_file_suffix=\".stats-v2\",task=None):\n",
    "    # TODO: recalculate missing stats if thresholds don't match the saved ones\n",
    "    \n",
    "    if thresholds is None:\n",
    "        thresholds = [0,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,\n",
    "              0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "    \n",
    "    stats_all = []\n",
    "    \n",
    "    for prediction_file in prediction_files:\n",
    "        stats_file = prediction_file[2] + stats_file_suffix\n",
    "        print(prediction_file)\n",
    "        if os.path.exists(stats_file):\n",
    "            print('Loading previously saved results.')\n",
    "            results,stats,results_pf = load_saved_results(stats_file)\n",
    "        else:\n",
    "            print('No saved results found. Starting evaluation - this may take some time...')\n",
    "            if prediction_file[0] == \"w2v2\":\n",
    "                results,stats,results_pf = evaluate_overlaps_w2v2(\n",
    "                        prediction_file[2],thresholds=thresholds,ref_suffix=ref_suffix,\n",
    "                        main_thresh=main_thresh,audio_id_list=audio_id_list,\n",
    "                        duration=duration,shift=shift,task=task\n",
    "                )\n",
    "            elif prediction_file[0] == \"CNN\":\n",
    "                results,stats,results_pf = evaluate_overlaps_CNN(\n",
    "                    prediction_file[2],frameRate=prediction_file[3],offset=prediction_file[4],\n",
    "                    thresholds=thresholds,audio_id_list=audio_id_list,task=task,ref_suffix=ref_suffix)\n",
    "            else:\n",
    "                print(\"Unrecognized results format. Skipping.\")\n",
    "                continue\n",
    "            save_results(stats_file,results,stats,results_pf)\n",
    "        stats_all.append({\"stats\":stats, \"name\": prediction_file[1]})\n",
    "        if results is not None:\n",
    "            print(tabulate(results, headers=\"firstrow\",tablefmt=\"simple\"))\n",
    "            print(\"\")\n",
    "        if results_pf is not None:\n",
    "            print(tabulate(results_pf, headers=\"firstrow\",tablefmt=\"simple\"))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    return stats_all\n",
    "\n",
    "def load_saved_results(stats_file):\n",
    "    \n",
    "    with open(stats_file, 'rb') as file:\n",
    "        results = pickle.load(file)\n",
    "        stats = pickle.load(file)\n",
    "        results_pf = pickle.load(file)\n",
    "    \n",
    "    return results,stats,results_pf\n",
    "    \n",
    "    \n",
    "def save_results(stats_file,results,stats,results_pf):\n",
    "    \n",
    "    with open(stats_file, 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "        pickle.dump(stats, file)\n",
    "        pickle.dump(results_pf, file)\n",
    "\n",
    "\n",
    "def evaluate_overlaps_w2v2(overlaps_file,thresholds=[0.5],main_thresh = 0.5,\n",
    "        frameRate=50,offset=0.02,audio_id_list=None,duration=None,\n",
    "        shift=None,ref_suffix=\"_overlaps.txt\",task=None):\n",
    "\n",
    "    hypotheses = []\n",
    "    \n",
    "    for threshold in thresholds: \n",
    "        hypotheses.append(\n",
    "            get_overlap_hypotheses_w2v2(\n",
    "                overlaps_file,threshold=threshold,audio_id_list=audio_id_list,duration=duration,shift=shift\n",
    "            )\n",
    "        )\n",
    " \n",
    "    if task[0:4] == \"AMI_\":\n",
    "        results,stats,results_pf = evaluate_AMI(hypotheses,thresholds,main_thresh,task)\n",
    "    else:\n",
    "        results,stats,results_pf = evaluate_overlaps(hypotheses,thresholds,main_thresh,ref_suffix=ref_suffix)\n",
    "    return results,stats,results_pf\n",
    "\n",
    "def evaluate_overlaps_CNN(overlaps_dir,frameRate,offset,thresholds=[0.5],main_thresh = 0.5,\n",
    "                          ref_suffix=\"_overlaps.txt\",audio_id_list=None,task=None):\n",
    "    # for CNN, frameRate and offset are required\n",
    "    \n",
    "    hypotheses = []\n",
    "    \n",
    "    for threshold in thresholds: \n",
    "        hypotheses.append(\n",
    "            get_overlap_hypotheses_CNN(\n",
    "                overlaps_dir,frameRate,offset,threshold=threshold,audio_id_list=audio_id_list\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    if task[0:4] == \"AMI_\":\n",
    "        results,stats,results_pf = evaluate_AMI(hypotheses,thresholds,main_thresh,task)\n",
    "    else:\n",
    "        results,stats,results_pf = evaluate_overlaps(hypotheses,thresholds,main_thresh,ref_suffix=ref_suffix)\n",
    "    return results,stats,results_pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeb44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot precision vs recall\n",
    "\n",
    "def make_plots(stats_all,xlims=[0,1],ylims=[0,1]):\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    for stat_dict in stats_all:\n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        plt.plot(stats[\"recall\"],stats[\"precision\"], label=name)\n",
    "    \n",
    "    plt.xlim(xlims)\n",
    "    plt.ylim(ylims)\n",
    "\n",
    "    plt.xlabel(\"recall\")\n",
    "    plt.ylabel(\"precision\")\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    for stat_dict in stats_all:\n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        thresholds = stats[\"thresholds\"]\n",
    "        plt.plot(thresholds,stats[\"precision\"], label=name + \" - precision\")\n",
    "        plt.plot(thresholds,stats[\"recall\"], label=name + \" - recall\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    for stat_dict in stats_all:\n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        plt.plot(thresholds,stats[\"precision\"], label=name + \" - precision\")\n",
    "        plt.plot(thresholds,stats[\"recall\"], label=name + \" - recall\")\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e505a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal threshold for each set of results\n",
    "\n",
    "# threshold where purity and coverage (or some other 2 stats) are closest to each other\n",
    "def get_closest_stats(stats_all, stat1 = \"precision\", stat2 = \"recall\", stat_main = \"fscore\"):\n",
    "    \n",
    "    best_indices = []\n",
    "    best_crit = []\n",
    "    \n",
    "    for stat_dict in stats_all:\n",
    "        \n",
    "        best_idx = None\n",
    "        best_diff = 9999999\n",
    "        \n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        thresholds = stats[\"thresholds\"]\n",
    "        \n",
    "        nThresh = len(thresholds)\n",
    "        for iThresh in range(nThresh): \n",
    "            diff = abs(stats[stat1][iThresh] - stats[stat2][iThresh])\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_idx = iThresh\n",
    "                \n",
    "        best_indices.append(best_idx)\n",
    "        best_crit.append(stats[stat_main][best_idx])\n",
    "        print(\"%s:\\n    %g (thresh %g)\"%(name,stats[stat_main][best_idx],thresholds[best_idx]))\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))  \n",
    "    #plt.bar([*range(1,len(best_crit)+1)],best_crit) \n",
    "    plt.plot(best_crit)\n",
    "\n",
    "# threshold where fscore (or some other stat) is highest\n",
    "def get_highest_stats(stats_all, stat_main = \"fscore\"):\n",
    "    \n",
    "    best_indices = []\n",
    "    best_crit = []\n",
    "    \n",
    "    for stat_dict in stats_all:\n",
    "        \n",
    "        best_idx = None\n",
    "        best_val = -9999999\n",
    "        \n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        thresholds = stats[\"thresholds\"]\n",
    "        \n",
    "        nThresh = len(thresholds)\n",
    "        for iThresh in range(nThresh): \n",
    "            val = stats[stat_main][iThresh]\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_idx = iThresh\n",
    "                \n",
    "        best_indices.append(best_idx)\n",
    "        best_crit.append(stats[stat_main][best_idx])\n",
    "        print(\"%s:\\n    %g (thresh %g)\"%(name,stats[stat_main][best_idx],thresholds[best_idx]))\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))  \n",
    "    #plt.bar([*range(1,len(best_crit)+1)],best_crit) \n",
    "    plt.plot(best_crit)\n",
    "\n",
    "# threshold where error_rate (or some other stat) is lowest\n",
    "def get_lowest_stats(stats_all, stat_main = \"error_rate\"):\n",
    "    \n",
    "    best_indices = []\n",
    "    best_crit = []\n",
    "    \n",
    "    for stat_dict in stats_all:\n",
    "        \n",
    "        best_idx = None\n",
    "        best_val = 9999999\n",
    "        \n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        thresholds = stats[\"thresholds\"]\n",
    "        \n",
    "        nThresh = len(thresholds)\n",
    "        for iThresh in range(nThresh): \n",
    "            val = stats[stat_main][iThresh]\n",
    "            if val < best_val:\n",
    "                best_val = val\n",
    "                best_idx = iThresh\n",
    "                \n",
    "        best_indices.append(best_idx)\n",
    "        best_crit.append(stats[stat_main][best_idx])\n",
    "        print(\"%s:\\n    %g (thresh %g)\"%(name,stats[stat_main][best_idx],thresholds[best_idx]))\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))  \n",
    "    #plt.bar([*range(1,len(best_crit)+1)],best_crit) \n",
    "    plt.plot(best_crit)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0526bfd",
   "metadata": {},
   "source": [
    "# Evaluation starts here\n",
    "pick one of the options and change the paths to your own, then run the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f061ca",
   "metadata": {},
   "source": [
    "## a) Evaluate OSD using custom references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176377d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set paths, thresholds, etc\n",
    "DATASET = 'AMI'\n",
    "\n",
    "REF_DIR = \"/path/to/directory/with/overlap/references/\"\n",
    "ref_suffix = \"_overlaps.txt\" # the expected filename format is (audio_id + ref_suffix), e.g. \"EN2002a_overlaps.txt\"\n",
    "\n",
    "thresholds = [0,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,\n",
    "              0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "\n",
    "mainDir_AMI_multitask = \"/storage/plzen4-ntis/projects/speaker_recog/AMI/Wav2vec2Transformer_multitask_OSD-VAD-SCD/\"\n",
    "\n",
    "task_id = 1 # our multitask model outputs predictions for 1. OSD, 2. VAD, 3. SCD; we want OSD here\n",
    "overlap_files_dev = []\n",
    "overlap_files_eval = []\n",
    "\n",
    "for epoch in range(1,7): # noSigmoid + base + auto weights\n",
    "    overlap_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI.task%d.txt\"%(epoch,task_id)])\n",
    "    overlap_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI-dev.task%d.txt\"%(epoch,task_id)])\n",
    "overlap_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI.task%d.txt\"%(task_id)])\n",
    "overlap_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI-dev.task%d.txt\"%(task_id)])\n",
    "  \n",
    "# duration and step size of audio segments (to make sure they are stitched back together correctly)\n",
    "duration = 20\n",
    "shift = 10\n",
    "\n",
    "main_thresh=0.5 # results for this threshold will be printed in greater detail\n",
    "    \n",
    "overlap_stats_all_dev = evaluate_all(\n",
    "    overlap_files_dev,thresholds=thresholds,main_thresh=main_thresh,ref_suffix=ref_suffix,duration=duration,shift=shift\n",
    ")\n",
    "overlap_stats_all_eval = evaluate_all(\n",
    "    overlap_files_eval,thresholds=thresholds,main_thresh=main_thresh,ref_suffix=ref_suffix,duration=duration,shift=shift\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6b852",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the best threshold for each set of results\n",
    "\n",
    "# a) highest F1\n",
    "print(\"dev set:\")\n",
    "get_highest_stats(overlap_stats_all_dev)\n",
    "\n",
    "print(\"test set:\")\n",
    "get_highest_stats(overlap_stats_all_eval)\n",
    "\n",
    "# b) most similar Cov and Pur\n",
    "#print(\"dev set:\")\n",
    "#get_closest_stats(overlap_stats_all_dev)\n",
    "#print(\"test set:\")\n",
    "#get_closest_stats(overlap_stats_all_eval)\n",
    "\n",
    "# plots\n",
    "# make_plots(overlap_stats_all_dev)\n",
    "# make_plots(overlap_stats_all_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdaaf6a",
   "metadata": {},
   "source": [
    "## b) Evaluate VAD using custom references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a46ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set paths, thresholds, etc\n",
    "DATASET = 'AMI'\n",
    "\n",
    "REF_DIR = \"/path/to/directory/with/VAD/references/\"\n",
    "ref_suffix = \".vad\" # the expected filename format is (audio_id + ref_suffix), e.g. \"EN2002a.vad\"\n",
    "\n",
    "thresholds = [0,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,\n",
    "              0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "\n",
    "mainDir_AMI_multitask = \"/storage/plzen4-ntis/projects/speaker_recog/AMI/Wav2vec2Transformer_multitask_OSD-VAD-SCD/\"\n",
    "\n",
    "# paths to the model outputs\n",
    "VAD_files_dev = []\n",
    "VAD_files_eval = []\n",
    "task_id = 2 # our multitask model outputs predictions for 1. OSD, 2. VAD, 3. SCD; we want VAD here\n",
    "for epoch in range(1,7): # noSigmoid + base + auto weights\n",
    "    VAD_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI.task%d.txt\"%(epoch,task_id)])\n",
    "    VAD_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI-dev.task%d.txt\"%(epoch,task_id)])\n",
    "VAD_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI.task%d.txt\"%(task_id)])\n",
    "VAD_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI-dev.task%d.txt\"%(task_id)])\n",
    "  \n",
    "# duration and step size of audio segments (to make sure they are stitched back together correctly)\n",
    "duration = 20\n",
    "shift = 10\n",
    "\n",
    "main_thresh=0.5 # results for this threshold will be printed in greater detail\n",
    "    \n",
    "VAD_stats_all_dev = evaluate_all(\n",
    "    VAD_files_dev,thresholds=thresholds,main_thresh=main_thresh,ref_suffix=ref_suffix,duration=duration,shift=shift\n",
    ")\n",
    "VAD_stats_all_eval = evaluate_all(\n",
    "    VAD_files_eval,thresholds=thresholds,main_thresh=main_thresh,ref_suffix=ref_suffix,duration=duration,shift=shift\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f5ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the best threshold for each set of results\n",
    "\n",
    "## a) highest F1\n",
    "# print(\"dev set:\")\n",
    "# get_highest_stats(VAD_stats_all_dev)\n",
    "# print(\"test set:\")\n",
    "# get_highest_stats(VAD_stats_all_eval)\n",
    "\n",
    "## b) most similar Cov and Pur\n",
    "#print(\"dev set:\")\n",
    "#get_closest_stats(VAD_stats_all_dev)\n",
    "#print(\"test set:\")\n",
    "#get_closest_stats(VAD_stats_all_eval)\n",
    "\n",
    "# c) lowest error:\n",
    "print(\"dev set:\")\n",
    "get_lowest_stats(VAD_stats_all_dev)\n",
    "print(\"test set:\")\n",
    "get_lowest_stats(VAD_stats_all_eval)\n",
    "\n",
    "# plots\n",
    "# make_plots(VAD_stats_all_dev)\n",
    "# make_plots(VAD_stats_all_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6381667e",
   "metadata": {},
   "source": [
    "## c) Evaluate AMI OSD and VAD using pyannote.db.odessa.ami\n",
    "(AMI corpus only)\n",
    "\n",
    "enabled by passing `task=\"AMI_OSD_test\"`, `\"AMI_OSD_dev\"`, `\"AMI_VAD_test\"`, or `\"AMI_VAD_dev\"` to `evaluate_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667ac88c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set paths, thresholds, etc\n",
    "\n",
    "DATASET = 'AMI'\n",
    "\n",
    "REF_DIR = None      # not used here\n",
    "ref_suffix = None   # not used here\n",
    "\n",
    "thresholds = [0,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,\n",
    "              0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "\n",
    "mainDir_AMI_multitask = \"/storage/plzen4-ntis/projects/speaker_recog/AMI/Wav2vec2Transformer_multitask_OSD-VAD-SCD/\"\n",
    "\n",
    "#----\n",
    "# OSD\n",
    "#----\n",
    "\n",
    "# paths to the model outputs\n",
    "task_id = 1 # our multitask model outputs predictions for 1. OSD, 2. VAD, 3. SCD; we want OSD here\n",
    "overlap_files_eval = []\n",
    "overlap_files_dev = []\n",
    "for epoch in range(1,7): # noSigmoid + base + auto weights\n",
    "    overlap_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI.task%d.txt\"%(epoch,task_id)])\n",
    "    overlap_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI-dev.task%d.txt\"%(epoch,task_id)])\n",
    "overlap_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI.task%d.txt\"%(task_id)])\n",
    "overlap_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI-dev.task%d.txt\"%(task_id)])\n",
    "\n",
    "# duration and step size of audio segments (to make sure they are stitched back together correctly)\n",
    "duration = 20\n",
    "shift = 10\n",
    "\n",
    "main_thresh=0.15 # results for this threshold will be printed in greater detail\n",
    "\n",
    "overlap_stats_all_dev_AMI = evaluate_all(\n",
    "    overlap_files_dev, thresholds=thresholds,\n",
    "    task=\"AMI_OSD_dev\", # this is the important part\n",
    "    stats_file_suffix='.dev.pyannote-refs.stats-v2',\n",
    "    main_thresh=main_thresh,duration=duration,shift=shift\n",
    ")\n",
    "\n",
    "overlap_stats_all_eval_AMI = evaluate_all(\n",
    "    overlap_files_eval, thresholds=thresholds,\n",
    "    task=\"AMI_OSD_test\", # this is the important part\n",
    "    stats_file_suffix='.eval.pyannote-refs.stats-v2',\n",
    "    main_thresh=main_thresh,duration=duration,shift=shift\n",
    ")\n",
    "\n",
    "#---\n",
    "# VAD\n",
    "#---\n",
    "\n",
    "# paths to the model outputs\n",
    "VAD_files_dev = []\n",
    "VAD_files_eval = []\n",
    "task_id = 2 # our multitask model outputs predictions for 1. OSD, 2. VAD, 3. SCD; we want VAD here\n",
    "for epoch in range(1,7): # noSigmoid + base + auto weights\n",
    "    VAD_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI.task%d.txt\"%(epoch,task_id)])\n",
    "    VAD_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI-dev.task%d.txt\"%(epoch,task_id)])\n",
    "VAD_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI.task%d.txt\"%(task_id)])\n",
    "VAD_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI-dev.task%d.txt\"%(task_id)])\n",
    "\n",
    "\n",
    "# duration and step size of audio segments (to make sure they are stitched back together correctly)\n",
    "duration = 20\n",
    "shift = 10\n",
    "\n",
    "main_thresh=0.4 # results for this threshold will be printed in greater detail\n",
    "\n",
    "VAD_stats_all_dev_AMI = evaluate_all(\n",
    "    VAD_files_dev, thresholds=thresholds,\n",
    "    task=\"AMI_VAD_dev\", # this is the important part\n",
    "    stats_file_suffix='.dev.pyannote-refs.stats-v2',\n",
    "    main_thresh=main_thresh,duration=duration,shift=shift\n",
    ")\n",
    "\n",
    "VAD_stats_all_eval_AMI = evaluate_all(\n",
    "    VAD_files_eval, thresholds=thresholds,\n",
    "    task=\"AMI_VAD_test\", # this is the important part\n",
    "    stats_file_suffix='.eval.pyannote-refs.stats-v2',\n",
    "    main_thresh=main_thresh,duration=duration,shift=shift\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1132f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"overlaps:\")\n",
    "\n",
    "get_highest_stats(overlap_stats_all_dev_AMI)\n",
    "get_highest_stats(overlap_stats_all_eval_AMI)\n",
    "\n",
    "print(\"\\nVAD:\")\n",
    "\n",
    "get_lowest_stats(VAD_stats_all_dev_AMI)\n",
    "get_lowest_stats(VAD_stats_all_eval_AMI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf90ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
