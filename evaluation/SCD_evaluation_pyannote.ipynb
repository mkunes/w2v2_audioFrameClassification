{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076d3a19",
   "metadata": {},
   "source": [
    "# Evaluate Speaker Change Detection (SCD)\n",
    "\n",
    "evaluate SCD predictions obtained via wav2vec2_audioFrameClassification.py or wav2vec2_audioFrameClassification_multitask.py\n",
    "\n",
    "evaluation is done via pyannote.metrics (https://pyannote.github.io/)\n",
    "\n",
    "evaluation on AMI corpus supports pyannote.db.odessa.ami (https://github.com/pyannote/pyannote-db-odessa-ami) - \n",
    "use task=\"AMI_SCD_dev\" (ODESSA/AMI development set) or task=\"AMI_SCD_test\" (ODESSA/AMI test set)\n",
    "\n",
    "other datasets require reference annotations in RTTM format (one RTTM file per audio)\n",
    "\n",
    "### How to use this notebook:\n",
    "- run all cells until you reach \"Evaluation starts here\"\n",
    "- pick one of the options for eval (custom references or pyannote.db)\n",
    "- change the paths and settings there and run the cells\n",
    "\n",
    "### Changelog:\n",
    "2023-03-09:\n",
    "  - fixed a mistake in the way audio chunks are concatenated in get_SCD_predictions_w2v2()  \n",
    "    (this affected the reported results, but the difference was minor - around the 4th significant digit in most cases)\n",
    "  - fixed a few edge cases that previously threw errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5129b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyannote.core import Annotation, Timeline, Segment\n",
    "from pyannote.metrics.segmentation import (\n",
    "    SegmentationCoverage,\n",
    "    SegmentationPurity,\n",
    "    SegmentationPurityCoverageFMeasure,\n",
    "    SegmentationPrecision,\n",
    "    SegmentationRecall\n",
    ")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4d770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads SCD references from RTTM format (for corpora other than AMI, or if using custom RTTM files)\n",
    "\n",
    "def get_SCD_reference(ref_file,frameRate=100,offset=0):\n",
    "    \n",
    "    reference = Annotation()\n",
    "    \n",
    "    maxTime = 0\n",
    "\n",
    "    with open(ref_file,'r') as file:\n",
    "        for line in file:\n",
    "            lineType,uri,_,startTime,dur,_,_,spk,_ = line.split(\" \",8)\n",
    "            startTime = float(startTime)\n",
    "            endTime = startTime + float(dur)\n",
    "            if endTime > maxTime:\n",
    "                maxTime = endTime\n",
    "            if lineType == \"SPEAKER\":\n",
    "                reference[Segment(startTime, endTime)] = spk\n",
    "\n",
    "    uem = Segment(0,endTime)\n",
    "     \n",
    "    return reference,uem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e527d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts filenames of audio segments into unique identifiers of the original audio files\n",
    "\n",
    "def get_audio_id(filename):\n",
    "    # obtains the ID of an audio file by repeatedly removing specific substrings from the end\n",
    "    #  (and only the end, to avoid false matches)\n",
    "    #  (e.g. \"IS1009c.Mix-Headset_t700-710.wav\" -> \"IS1009c\")\n",
    "    basename = os.path.basename(filename)\n",
    "    \n",
    "    suffixesToRemove = ['.wav','.mp4','.dereverb','.denoise','.OMEETING',\n",
    "                        '.OHALLWAY','.OOFFICE','.meeting','.booth','.office',\n",
    "                        '_OMEETING','_OHALLWAY','_OOFFICE','_meeting','_booth','_office',\n",
    "                        '.Mix-Headset'] \n",
    "    \n",
    "    regexToRemove = ['(\\.|_)\\d+kHz',\n",
    "                    '_t[0-9]+(\\.[0-9]+)?\\-[0-9]+(\\.[0-9]+)?',\n",
    "                    '_\\d{5}']\n",
    "    \n",
    "    found = True\n",
    "    while found:\n",
    "        found = False\n",
    "        for suffix in suffixesToRemove:\n",
    "            if basename.endswith(suffix):\n",
    "                basename = basename[:-(len(suffix))]\n",
    "                found = True\n",
    "        for rgx in regexToRemove:\n",
    "            match = re.search(rgx+\"$\",basename) # +\"$\" ensures we only find a match at the *end* \n",
    "            if match is not None:\n",
    "                basename = basename[:match.start()]\n",
    "                found = True\n",
    "    \n",
    "    return basename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88906317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get SCD hypotheses for each threshold, in pyannote's preferred format\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "def get_annotation_from_labels(labels,frameRate,offset=0.02,threshold=0.5):\n",
    "    num_values = len(labels)\n",
    "    \n",
    "    annotation = Annotation()\n",
    "        \n",
    "    # find peaks\n",
    "    peak_indices,_ = find_peaks(labels,height=threshold)\n",
    "    #print(\"Peaks:\")\n",
    "    #print(peak_indices)\n",
    "    \n",
    "    changes = []\n",
    "    for peak_idx in peak_indices:\n",
    "        changes.append(peak_idx / frameRate + offset)\n",
    "    changes.append(num_values / frameRate + offset)\n",
    "    \n",
    "    #print(\"Changes:\")\n",
    "    #print(changes)\n",
    "    \n",
    "    startTime = 0\n",
    "    for endTime in changes:\n",
    "        annotation[Segment(startTime, endTime)] = 'segment'\n",
    "        startTime = endTime\n",
    "        \n",
    "    return annotation\n",
    "\n",
    "def get_SCD_hypotheses_w2v2(labels_all,frameRate=50,threshold=0.5,offset=0.02):\n",
    "    # turns predicted labels into segment hypotheses in a format usable by pyannote\n",
    "    \n",
    "    hypotheses = {}\n",
    "    for audio_id in labels_all:\n",
    "        hypotheses[audio_id] = get_annotation_from_labels(labels_all[audio_id],\n",
    "                frameRate=frameRate,offset=offset,threshold=threshold)\n",
    "    \n",
    "    return hypotheses\n",
    "\n",
    "def get_SCD_predictions_w2v2(test_file,frameRate=50,audio_id_list=None,duration=None,shift=None):\n",
    "    # reads predicted labels from w2v2 output, stitches them back together\n",
    "    \n",
    "    predictions_all = []\n",
    "    durations = {}\n",
    "    \n",
    "    with open(test_file,'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            path,values_str = line.split(\",\")\n",
    "            audio_filename = os.path.basename(path)\n",
    "            \n",
    "            audio_id = get_audio_id(audio_filename)\n",
    "            if (audio_id_list is not None) and not (audio_id in audio_id_list):\n",
    "                continue # skip files that are not on the list\n",
    "            \n",
    "            match = re.search('_t[0-9]+(\\.[0-9]+)?\\-[0-9]+(\\.[0-9]+)?',audio_filename)\n",
    "            if match is not None:\n",
    "                str = match.group()\n",
    "                startTime,endTime = str[2:].split('-')\n",
    "                startTime = float(startTime)\n",
    "                endTime = float(endTime)\n",
    "            else:\n",
    "                # TODO: do not require start and end times if there's only one segment per audio file\n",
    "                raise ValueError('Could not determine the start time and end time of audio chunks')\n",
    "                #startTime = None\n",
    "                #endTime = None\n",
    "\n",
    "            idx1 = values_str.find('[')\n",
    "            idx2 = values_str.find(']')\n",
    "            \n",
    "            values = values_str[idx1+1:idx2].split(\" \")\n",
    "            values = list(map(float, values))\n",
    "            \n",
    "            predictions_all.append({\n",
    "                \"path\":path,\n",
    "                \"values\": values,\n",
    "                \"id\": audio_id,\n",
    "                \"startTime\": startTime,\n",
    "                \"endTime\": endTime\n",
    "            })\n",
    "            \n",
    "            if (audio_id not in durations) or (endTime < durations[audio_id]):\n",
    "                durations[audio_id] = endTime\n",
    "            \n",
    "    \n",
    "    labels_all = {}\n",
    "    for prediction in predictions_all:\n",
    "        audio_id = prediction[\"id\"]\n",
    "        \n",
    "        startTime = prediction[\"startTime\"]\n",
    "        endTime = prediction[\"endTime\"]\n",
    "                    \n",
    "        if audio_id not in labels_all:\n",
    "            if startTime is None:\n",
    "                # Note: \"+ [0]\" adds the one missing label (w2v outputs only 999 labels for 20s audio)\n",
    "                labels_all[audio_id] = prediction[\"values\"] + [0]\n",
    "            else:\n",
    "                labels_all[audio_id] = [0] * round(durations[audio_id] * frameRate)\n",
    "        \n",
    "        if endTime is not None:\n",
    "\n",
    "            startIdx = round(startTime * frameRate)\n",
    "            endIdx = round(endTime * frameRate)\n",
    "\n",
    "            if duration is not None and shift is not None:\n",
    "                overlap = duration - shift # this \"overlap\" refers to overlap between audio segments\n",
    "                halfoverlap_frames = round(shift * frameRate / 2)\n",
    "                startIdx = startIdx + halfoverlap_frames\n",
    "\n",
    "                if startIdx >= endIdx: # this can happen in the very last segment\n",
    "                    #print(\"start time %f, end time %f\"%(startTime,endTime))\n",
    "                    continue # just skip this file completely\n",
    "\n",
    "                newLabels = prediction[\"values\"][halfoverlap_frames:]\n",
    "\n",
    "                if endIdx - startIdx != len(newLabels):\n",
    "                    # w2v labels are one value shorter than time*frameRate => off by one will probably happen every time...\n",
    "                    #  ... and the last segment of each conversation will obviously have a much bigger difference\n",
    "                    endIdx = startIdx + len(newLabels)\n",
    "                labels_all[audio_id][startIdx:endIdx] = newLabels\n",
    "            else:\n",
    "                if len(prediction[\"values\"]) != endIdx - startIdx:\n",
    "                    endIdx = startIdx + len(prediction[\"values\"])\n",
    "                labels_all[audio_id][startIdx:endIdx] = prediction[\"values\"]\n",
    "        else:\n",
    "            # currently cannot happen - either both are None or neither is\n",
    "            startIdx = round(startTime * frameRate)\n",
    "            labels_all[audio_id][startIdx:] = prediction[\"values\"]\n",
    "\n",
    "            \n",
    "    return labels_all\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31fc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coverage, purity, etc for all files and thresholds from a specific set of hypotheses\n",
    "#  (version with custom RTTM references)\n",
    "def evaluate_SCD(hypotheses,thresholds,main_thresh,pr_tolerance=None,pc_tolerance=None):\n",
    "    \n",
    "    nThresh = len(thresholds)\n",
    "    \n",
    "    if pr_tolerance is None:\n",
    "        segmentationPrecision = SegmentationPrecision()\n",
    "        segmentationRecall = SegmentationRecall()\n",
    "    else:\n",
    "        segmentationPrecision = SegmentationPrecision(tolerance=pr_tolerance)\n",
    "        segmentationRecall = SegmentationRecall(tolerance=pr_tolerance)\n",
    "        \n",
    "    if pc_tolerance is None:\n",
    "        segmentationCoverage = SegmentationCoverage()\n",
    "        segmentationPurity = SegmentationPurity()\n",
    "        segmentationFMeasure = SegmentationPurityCoverageFMeasure()\n",
    "    else:\n",
    "        segmentationCoverage = SegmentationCoverage(tolerance=pc_tolerance)\n",
    "        segmentationPurity = SegmentationPurity(tolerance=pc_tolerance)\n",
    "        segmentationFMeasure = SegmentationPurityCoverageFMeasure(tolerance=pc_tolerance)\n",
    "    \n",
    "    results = [[\"threshold\",\"coverage\",\"purity\",\"precision\",\"recall\",\"fscore\"]]\n",
    "    results_pf = [[\"audio_id\",\"coverage\",\"purity\",\"precision\",\"recall\",\"fscore\"]]\n",
    "    reference_all = None\n",
    "    \n",
    "    \n",
    "    cov_dur_total = [0] * nThresh\n",
    "    pur_dur_total = [0] * nThresh\n",
    "    all_dur_total = [0] * nThresh\n",
    "    \n",
    "    fscore_avg = [0] * nThresh\n",
    "    \n",
    "    precision_avg = [0] * nThresh\n",
    "    prec_matches_total = [0] * nThresh\n",
    "    prec_bounds_total = [0] * nThresh\n",
    "    \n",
    "    recall_avg = [0] * nThresh\n",
    "    rec_matches_total = [0] * nThresh\n",
    "    rec_bounds_total = [0] * nThresh\n",
    "    \n",
    "    purity_avg = [0] * nThresh\n",
    "    coverage_avg = [0] * nThresh\n",
    "    \n",
    "\n",
    "    precision_total = [0] * nThresh\n",
    "    recall_total = [0] * nThresh\n",
    "    fscore_total = [0] * nThresh\n",
    "    coverage_total = [0] * nThresh\n",
    "    purity_total = [0] * nThresh\n",
    "    \n",
    "    nfiles = 0\n",
    "\n",
    "    for audio_id in hypotheses[0].keys():\n",
    "        #print(\"\\n\")\n",
    "        print(audio_id)\n",
    "        \n",
    "        ref_file = os.path.join(REF_DIR, audio_id + RTTM_SUFFIX)\n",
    "        if not os.path.exists(ref_file):\n",
    "            print('File ''%s'' not found. Skipping.'%ref_file)\n",
    "            continue\n",
    "        \n",
    "        nfiles = nfiles + 1\n",
    "        \n",
    "        reference,uem = get_SCD_reference(ref_file,frameRate=100,offset=0)\n",
    "        \n",
    "        for iThresh in range(nThresh): \n",
    "\n",
    "            hypothesis = hypotheses[iThresh][audio_id]\n",
    "            \n",
    "            # Coverage\n",
    "            coverage_details = segmentationCoverage(reference, hypothesis, detailed=True, uem=uem)\n",
    "            coverage = coverage_details[\"segmentation coverage\"]\n",
    "            cov_dur_total[iThresh] = cov_dur_total[iThresh] + coverage_details[\"intersection duration\"]\n",
    "            all_dur_total[iThresh] = all_dur_total[iThresh] + coverage_details[\"total duration\"]\n",
    "            \n",
    "            coverage_avg[iThresh] = coverage_avg[iThresh] + coverage\n",
    "            \n",
    "            # Purity\n",
    "            purity_details = segmentationPurity(reference, hypothesis, detailed=True, uem=uem)\n",
    "            purity = purity_details[\"segmentation purity\"]\n",
    "            pur_dur_total[iThresh] = pur_dur_total[iThresh] + purity_details[\"intersection duration\"]\n",
    "            \n",
    "            purity_avg[iThresh] = purity_avg[iThresh] + purity\n",
    "            \n",
    "            # F-score\n",
    "            fscore_details = segmentationFMeasure(reference, hypothesis, detailed=True, uem=uem)\n",
    "            fscore = fscore_details[\"segmentation F[purity|coverage]\"]\n",
    "            fscore_avg[iThresh] = fscore_avg[iThresh] + fscore\n",
    "\n",
    "\n",
    "            # Precision\n",
    "            precision_details = segmentationPrecision(reference,hypothesis,detailed=True,uem=uem)\n",
    "            precision = precision_details[\"segmentation precision\"]\n",
    "            prec_matches_total[iThresh] = prec_matches_total[iThresh] + precision_details[\"number of matches\"]\n",
    "            prec_bounds_total[iThresh] = prec_bounds_total[iThresh] + precision_details[\"number of boundaries\"]\n",
    "            precision_avg[iThresh] = precision_avg[iThresh] + precision\n",
    "\n",
    "            # Recall\n",
    "\n",
    "            recall_details = segmentationRecall(reference,hypothesis,detailed=True,uem=uem)\n",
    "            recall = recall_details[\"segmentation recall\"]\n",
    "            rec_matches_total[iThresh] = rec_matches_total[iThresh] + recall_details[\"number of matches\"]\n",
    "            rec_bounds_total[iThresh] = rec_bounds_total[iThresh] + recall_details[\"number of boundaries\"]\n",
    "            recall_avg[iThresh] = recall_avg[iThresh] + recall\n",
    "\n",
    "            if thresholds[iThresh] == main_thresh:\n",
    "                results_pf.append([audio_id,coverage,purity,precision,recall,fscore])\n",
    "\n",
    "    \n",
    "    for iThresh in range(nThresh): \n",
    "        \n",
    "        fscore_avg[iThresh] = fscore_avg[iThresh] / nfiles\n",
    "        precision_avg[iThresh] = precision_avg[iThresh] / nfiles\n",
    "        recall_avg[iThresh] = recall_avg[iThresh] / nfiles\n",
    "        purity_avg[iThresh] = purity_avg[iThresh] / nfiles\n",
    "        coverage_avg[iThresh] = coverage_avg[iThresh] / nfiles\n",
    "        \n",
    "        if prec_bounds_total[iThresh] == 0:\n",
    "            precision_total[iThresh] = 1\n",
    "        else:\n",
    "            precision_total[iThresh] = prec_matches_total[iThresh] / prec_bounds_total[iThresh]\n",
    "        \n",
    "        if rec_bounds_total[iThresh] == 0:\n",
    "            recall_total[iThresh] = 1\n",
    "        else:\n",
    "            recall_total[iThresh] = rec_matches_total[iThresh] / rec_bounds_total[iThresh]\n",
    "        \n",
    "        if all_dur_total[iThresh] == 0:\n",
    "            coverage_total[iThresh] = 1\n",
    "            purity_total[iThresh] = 1\n",
    "        else: \n",
    "            coverage_total[iThresh] = cov_dur_total[iThresh] / all_dur_total[iThresh]\n",
    "            purity_total[iThresh] = pur_dur_total[iThresh] / all_dur_total[iThresh]\n",
    "            \n",
    "        if (coverage_total[iThresh] + purity_total[iThresh]) == 0:\n",
    "            fscore_total[iThresh] = 0\n",
    "        else:\n",
    "            fscore_total[iThresh] = 2 * (coverage_total[iThresh] * purity_total[iThresh]) / (coverage_total[iThresh] + purity_total[iThresh])\n",
    "        \n",
    "        if thresholds[iThresh] == main_thresh:\n",
    "            results_pf.append([\"AVG\",coverage_avg[iThresh],purity_avg[iThresh],precision_avg[iThresh],recall_avg[iThresh],fscore_avg[iThresh]])\n",
    "            results_pf.append([\"TOTAL\",coverage_total[iThresh],purity_total[iThresh],precision_total[iThresh],recall_total[iThresh],fscore_total[iThresh]])\n",
    "\n",
    "        results.append([thresholds[iThresh],coverage_total[iThresh],purity_total[iThresh],precision_total[iThresh],recall_total[iThresh],fscore_total[iThresh]])\n",
    "    \n",
    "    stats = {}\n",
    "    stats[\"thresholds\"] = thresholds\n",
    "    stats[\"coverage\"] = coverage_total\n",
    "    stats[\"purity\"] = purity_total\n",
    "    stats[\"precision\"] = precision_total\n",
    "    stats[\"recall\"] = recall_total\n",
    "    stats[\"fscore\"] = fscore_total\n",
    "    \n",
    "    return results, stats, results_pf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate coverage, purity, etc for all files and thresholds from a specific set of hypotheses\n",
    "#  (version using pyannote.db.odessa.ami, AMI corpus only)\n",
    "\n",
    "from pyannote.database import get_protocol, FileFinder\n",
    "\n",
    "def evaluate_SCD_AMI(hypotheses,thresholds,main_thresh,task):\n",
    "    \n",
    "    print('Evaluation with pyannote refs')\n",
    "    \n",
    "    #preprocessors = {'audio': FileFinder()}\n",
    "    protocol = get_protocol('AMI.SpeakerDiarization.MixHeadset')\n",
    "        \n",
    "    segmentationCoverage = SegmentationCoverage()\n",
    "    segmentationPurity = SegmentationPurity()\n",
    "    segmentationFMeasure = SegmentationPurityCoverageFMeasure()\n",
    "    \n",
    "    coverage_total = []\n",
    "    purity_total = []\n",
    "    fscore_total = []\n",
    "    \n",
    "    results = [[\"threshold\",\"coverage\",\"purity\",\"fscore\"]]\n",
    "    \n",
    "    nThresh = len(thresholds)\n",
    "    for iThresh in range(nThresh): \n",
    "        if thresholds[iThresh] == main_thresh: \n",
    "            display = True\n",
    "            print(\"Results for threshold %g:\"%main_thresh)\n",
    "        else:\n",
    "            display = False\n",
    "\n",
    "        if task[-4:] == \"_dev\":\n",
    "            AMI_set = protocol.development()\n",
    "        else:\n",
    "            AMI_set = protocol.test()\n",
    "            \n",
    "        # iterate over each file of the test/dev set\n",
    "        for test_file in AMI_set:\n",
    "            \n",
    "            audio_id = get_audio_id(test_file[\"uri\"])\n",
    "            #print(audio_id)\n",
    "            \n",
    "            hypothesis = hypotheses[iThresh][audio_id]\n",
    "            \n",
    "            # evaluate hypothesis\n",
    "            reference = test_file['annotation']\n",
    "            \n",
    "            uem = test_file['annotated']\n",
    "            segmentationCoverage(reference, hypothesis, uem=uem)\n",
    "            segmentationPurity(reference, hypothesis, uem=uem)\n",
    "            segmentationFMeasure(reference, hypothesis, uem=uem)\n",
    "            \n",
    "        cov = segmentationCoverage.report(display=display)\n",
    "        pur = segmentationPurity.report(display=display)\n",
    "        F = segmentationFMeasure.report(display=display)\n",
    "        \n",
    "        cov = cov.values.tolist()\n",
    "        coverage_total.append(cov[-1][0])\n",
    "        \n",
    "        pur = pur.values.tolist()\n",
    "        purity_total.append(pur[-1][0])\n",
    "        \n",
    "        F = F.values.tolist()\n",
    "        fscore_total.append(F[-1][0])\n",
    "        \n",
    "        results.append(\n",
    "            [thresholds[iThresh],coverage_total[iThresh],purity_total[iThresh],fscore_total[iThresh]]\n",
    "        )\n",
    "        \n",
    "        segmentationCoverage.reset()\n",
    "        segmentationPurity.reset()\n",
    "        segmentationFMeasure.reset()\n",
    "        \n",
    "    stats = {}\n",
    "    stats[\"thresholds\"] = thresholds\n",
    "    stats[\"coverage\"] = coverage_total\n",
    "    stats[\"purity\"] = purity_total\n",
    "    stats[\"fscore\"] = fscore_total\n",
    "        \n",
    "    return results,stats,None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f827673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate multiple sets of predictions\n",
    "#   (stats are saved so that they don't need to be recalculated every time)\n",
    "# TODO: check if thresholds match the saved ones, recalculate (only) the missing stats if not\n",
    "\n",
    "import pickle\n",
    "\n",
    "def evaluate_all(prediction_files, thresholds=None,\n",
    "                 main_thresh=0.25,audio_id_list=None,duration=20,shift=10,task=None,\n",
    "                 stats_file_suffix='.stats'):\n",
    "    # TODO: recalculate missing stats if thresholds don't match the saved ones\n",
    "    \n",
    "    if thresholds is None:\n",
    "        thresholds = [0,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,\n",
    "              0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "    \n",
    "    stats_all = []\n",
    "    \n",
    "    for prediction_file in prediction_files:\n",
    "        stats_file = prediction_file[2] + stats_file_suffix\n",
    "        print(prediction_file)\n",
    "        if os.path.exists(stats_file):\n",
    "            print('Loading previously saved results.')\n",
    "            results,stats,results_pf = load_saved_results(stats_file)\n",
    "        else:\n",
    "            print('No saved results found. Starting evaluation - this may take some time...')\n",
    "            if prediction_file[0] == \"w2v2\":\n",
    "                results,stats,results_pf = evaluate_SCD_w2v2(prediction_file[2],thresholds=thresholds,\n",
    "                                                    main_thresh=main_thresh,audio_id_list=audio_id_list,\n",
    "                                                    duration=duration,shift=shift,task=task)\n",
    "            else:\n",
    "                print(\"Unrecognized results format ''%s''. Skipping.\"%prediction_file[0])\n",
    "                continue\n",
    "            save_results(stats_file,results,stats,results_pf)\n",
    "        stats_all.append({\"stats\":stats, \"name\": prediction_file[1]})\n",
    "        if results is not None:\n",
    "            print(tabulate(results, headers=\"firstrow\",tablefmt=\"simple\"))\n",
    "            print(\"\")\n",
    "        if results_pf is not None:\n",
    "            print(tabulate(results_pf, headers=\"firstrow\",tablefmt=\"simple\"))\n",
    "            print(\"\\n\")\n",
    "\n",
    "    return stats_all\n",
    "\n",
    "def load_saved_results(stats_file):\n",
    "    \n",
    "    with open(stats_file, 'rb') as file:\n",
    "        results = pickle.load(file)\n",
    "        stats = pickle.load(file)\n",
    "        results_pf = pickle.load(file)\n",
    "    \n",
    "    return results,stats,results_pf\n",
    "    \n",
    "    \n",
    "def save_results(stats_file,results,stats,results_pf):\n",
    "    \n",
    "    with open(stats_file, 'wb') as file:\n",
    "        pickle.dump(results, file)\n",
    "        pickle.dump(stats, file)\n",
    "        pickle.dump(results_pf, file)\n",
    "        \n",
    "def evaluate_SCD_w2v2(SCD_file,thresholds=[0.5],main_thresh = 0.5,\n",
    "        frameRate=50,offset=0.02,audio_id_list=None,duration=None,\n",
    "        shift=None,pr_tolerance=None,pc_tolerance=None,task=None):\n",
    "    \n",
    "    hypotheses = []\n",
    "    \n",
    "    labels_all = get_SCD_predictions_w2v2(\n",
    "        SCD_file,frameRate=frameRate,audio_id_list=audio_id_list,duration=duration,shift=shift\n",
    "    )\n",
    "    \n",
    "    for threshold in thresholds: \n",
    "        hypotheses.append(get_SCD_hypotheses_w2v2(labels_all,frameRate=frameRate,threshold=threshold,offset=offset))\n",
    "\n",
    "    if task is not None and task[0:4] == \"AMI_\":\n",
    "        results,stats,results_pf = evaluate_SCD_AMI(hypotheses,thresholds,main_thresh,task)\n",
    "    else:\n",
    "        results,stats,results_pf = evaluate_SCD(hypotheses,thresholds,main_thresh,pr_tolerance=pr_tolerance,pc_tolerance=pc_tolerance)\n",
    "    return results,stats,results_pf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot coverage vs purity\n",
    "\n",
    "def make_plots(stats_all,xlims=[0,1],ylims=[0,1]):\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    for stat_dict in stats_all:\n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        plt.plot(stats[\"coverage\"],stats[\"purity\"], label=name)\n",
    "    \n",
    "    plt.xlim(xlims)\n",
    "    plt.ylim(ylims)\n",
    "\n",
    "    plt.xlabel(\"coverage\")\n",
    "    plt.ylabel(\"purity\")\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    for stat_dict in stats_all:\n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        thresholds = stats[\"thresholds\"]\n",
    "        plt.plot(thresholds,stats[\"purity\"], label=name + \" - purity\")\n",
    "        plt.plot(thresholds,stats[\"coverage\"], label=name + \" - coverage\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    for stat_dict in stats_all:\n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        thresholds = stats[\"thresholds\"]\n",
    "        plt.plot(thresholds,stats[\"purity\"], label=name + \" - purity\")\n",
    "        plt.plot(thresholds,stats[\"coverage\"], label=name + \" - coverage\")\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1,1), loc=\"upper left\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b70914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the optimal threshold for each set of results\n",
    "\n",
    "# threshold where purity and coverage (or some other 2 stats) are closest to each other\n",
    "def get_closest_stats(stats_all, stat1 = \"purity\", stat2 = \"coverage\", stat_main = \"fscore\"):\n",
    "    \n",
    "    best_indices = []\n",
    "    best_crit = []\n",
    "    \n",
    "    for stat_dict in stats_all:\n",
    "        \n",
    "        best_idx = None\n",
    "        best_diff = 9999999\n",
    "        \n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        thresholds = stats[\"thresholds\"]\n",
    "        \n",
    "        nThresh = len(thresholds)\n",
    "        for iThresh in range(nThresh): \n",
    "            diff = abs(stats[stat1][iThresh] - stats[stat2][iThresh])\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_idx = iThresh\n",
    "                \n",
    "        best_indices.append(best_idx)\n",
    "        best_crit.append(stats[stat_main][best_idx])\n",
    "        print(\"%s:\\n    %g (thresh %g)\"%(name,stats[stat_main][best_idx],thresholds[best_idx]))\n",
    "        \n",
    "    plt.figure(figsize=(6, 6))    \n",
    "    #plt.bar([*range(1,len(best_crit)+1)],best_crit) \n",
    "    plt.plot(best_crit)\n",
    "    \n",
    "# threshold where fscore (or some other stat) is highest\n",
    "def get_highest_stats(stats_all, stat_main = \"fscore\"):\n",
    "    \n",
    "    best_indices = []\n",
    "    best_crit = []\n",
    "    \n",
    "    for stat_dict in stats_all:\n",
    "        \n",
    "        best_idx = None\n",
    "        best_val = -9999999\n",
    "        \n",
    "        stats = stat_dict[\"stats\"]\n",
    "        name = stat_dict[\"name\"]\n",
    "        thresholds = stats[\"thresholds\"]\n",
    "        \n",
    "        nThresh = len(thresholds)\n",
    "        for iThresh in range(nThresh): \n",
    "            val = stats[stat_main][iThresh]\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_idx = iThresh\n",
    "                \n",
    "        best_indices.append(best_idx)\n",
    "        best_crit.append(stats[stat_main][best_idx])\n",
    "        print(\"%s:\\n    %g (thresh %g)\"%(name,stats[stat_main][best_idx],thresholds[best_idx]))\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))  \n",
    "    #plt.bar([*range(1,len(best_crit)+1)],best_crit) \n",
    "    plt.plot(best_crit)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf01d23",
   "metadata": {},
   "source": [
    "# Evaluation starts here\n",
    "pick one of the options and change the paths to your own, then run the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04411a07",
   "metadata": {},
   "source": [
    "## a) Evaluate SCD using custom RTTM references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971b9680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set paths, thresholds, etc\n",
    "DATASET = 'AMI'\n",
    "\n",
    "REF_DIR = \"/path/to/directory/with/RTTM/files/\"\n",
    "RTTM_SUFFIX = \".rttm\" # the expected filename format is (audio_id + RTTM_SUFFIX), e.g. \"EN2002a.rttm\"\n",
    "\n",
    "thresholds = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "\n",
    "audio_id_list = None # use everything in the model outputs\n",
    "\n",
    "# audio_id_list = [\"EN2002a\",\"EN2002b\",\"EN2002c\",\"EN2002d\",\n",
    "#                  \"ES2004a\",\"ES2004b\",\"ES2004c\",\"ES2004d\",\n",
    "#                  \"ES2014a\",\"ES2014b\",\"ES2014c\",\"ES2014d\",\n",
    "#                  \"IS1009a\",\"IS1009b\",\"IS1009c\",\"IS1009d\",\n",
    "#                  \"TS3003a\",\"TS3003b\",\"TS3003c\",\"TS3003d\",\n",
    "#                  \"TS3007a\",\"TS3007b\",\"TS3007c\",\"TS3007d\"]\n",
    "\n",
    "mainDir_AMI_multitask = \"/storage/plzen4-ntis/projects/speaker_recog/AMI/Wav2vec2Transformer_multitask_OSD-VAD-SCD/\"\n",
    "\n",
    "# paths to the model outputs\n",
    "task_id = 3 # our multitask model outputs predictions for 1. OSD, 2. VAD, 3. SCD; we want SCD here\n",
    "SCD_files_eval = []\n",
    "SCD_files_dev = []\n",
    "for epoch in range(1,7): # noSigmoid + base + auto weights\n",
    "    SCD_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI.task%d.txt\"%(epoch,task_id)])\n",
    "    SCD_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI-dev.task%d.txt\"%(epoch,task_id)])\n",
    "SCD_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI.task%d.txt\"%(task_id)])\n",
    "SCD_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI-dev.task%d.txt\"%(task_id)])\n",
    "\n",
    "# duration and step size of audio segments (to make sure they are stitched back together correctly)\n",
    "duration = 20\n",
    "shift = 10\n",
    "\n",
    "main_thresh=0.25 # results for this threshold will be printed in greater detail\n",
    "\n",
    "# run evaluation\n",
    "stats_all_dev = evaluate_all(SCD_files_dev, thresholds=thresholds,main_thresh=main_thresh,audio_id_list=None,duration=duration,shift=shift)\n",
    "stats_all_eval = evaluate_all(SCD_files_eval, thresholds=thresholds,main_thresh=main_thresh,audio_id_list=None,duration=duration,shift=shift)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f73e624",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the best threshold for each set of results\n",
    "\n",
    "# a) highest F1\n",
    "print(\"dev set:\")\n",
    "get_highest_stats(stats_all_dev)\n",
    "\n",
    "print(\"test set:\")\n",
    "get_highest_stats(stats_all_eval)\n",
    "\n",
    "# b) most similar Cov and Pur\n",
    "#print(\"dev set:\")\n",
    "#get_closest_stats(stats_all_dev)\n",
    "#print(\"test set:\")\n",
    "#get_closest_stats(stats_all_eval)\n",
    "\n",
    "# plots\n",
    "# make_plots(stats_all_dev)\n",
    "# make_plots(stats_all_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f41eebf",
   "metadata": {},
   "source": [
    "## b) Evaluate AMI SCD using pyannote.db.odessa.ami\n",
    "(AMI corpus only)\n",
    "\n",
    "enabled by passing `task=\"AMI_SCD_test\"` or `task=\"AMI_SCD_dev\"` to `evaluate_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a209c10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set paths, thresholds, etc\n",
    "\n",
    "DATASET = 'AMI'\n",
    "\n",
    "REF_DIR = None      # not used here\n",
    "ref_suffix = None   # not used here\n",
    "\n",
    "thresholds = [0,0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n",
    "\n",
    "mainDir_AMI_multitask = \"/storage/plzen4-ntis/projects/speaker_recog/AMI/Wav2vec2Transformer_multitask_OSD-VAD-SCD/\"\n",
    "\n",
    "# paths to the model outputs\n",
    "task_id = 3 # our multitask model outputs predictions for 1. OSD, 2. VAD, 3. SCD; we want SCD here\n",
    "SCD_files_eval = []\n",
    "SCD_files_dev = []\n",
    "for epoch in range(1,7): # noSigmoid + base + auto weights\n",
    "    SCD_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI.task%d.txt\"%(epoch,task_id)])\n",
    "    SCD_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch %d\"%epoch,mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/epoch%d/output_AMI-dev.task%d.txt\"%(epoch,task_id)])\n",
    "SCD_files_eval.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - eval epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI.task%d.txt\"%(task_id)])\n",
    "SCD_files_dev.append([\"w2v2\",\"AMI multi noSigmoid autoWeights - dev epoch 7\",mainDir_AMI_multitask + \"AMITrain_fuzzy0.2_22-09-22_20s10s_7epochs_noSigmoid_yesWeights_seed1234/output_AMI-dev.task%d.txt\"%(task_id)])\n",
    "\n",
    "# duration and step size of audio segments (to make sure they are stitched back together correctly)\n",
    "duration = 20\n",
    "shift = 10\n",
    "\n",
    "main_thresh=0.25 # results for this threshold will be printed in greater detail\n",
    "\n",
    "# run evaluation\n",
    " \n",
    "stats_all_eval_AMI = evaluate_all(\n",
    "    SCD_files_eval, thresholds=thresholds,\n",
    "    task=\"AMI_SCD_test\", # this is the important part\n",
    "    stats_file_suffix='.pyannote-refs.thresh0.35.stats',\n",
    "    main_thresh=0.35,duration=duration,shift=shift\n",
    ")\n",
    "\n",
    "stats_all_dev_AMI = evaluate_all(\n",
    "    SCD_files_dev, thresholds=thresholds,\n",
    "    task=\"AMI_SCD_dev\", # this is the important part\n",
    "    stats_file_suffix='.pyannote-refs.thresh0.35.stats',\n",
    "    main_thresh=0.35,duration=duration,shift=shift\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e530e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get the best threshold for each set of results\n",
    "\n",
    "# a) highest F1\n",
    "print(\"dev set:\")\n",
    "get_highest_stats(stats_all_dev_AMI)\n",
    "\n",
    "print(\"test set:\")\n",
    "get_highest_stats(stats_all_eval_AMI)\n",
    "\n",
    "# b) most similar Cov and Pur\n",
    "#print(\"dev set:\")\n",
    "#get_closest_stats(stats_all_dev)\n",
    "#print(\"test set:\")\n",
    "#get_closest_stats(stats_all_eval)\n",
    "\n",
    "# plots\n",
    "# make_plots(stats_all_dev)\n",
    "# make_plots(stats_all_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7595f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
